{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"using-word-embeddings-by-use-zipfile.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### 원본 IMDB 텍스트 다운로드하기\n","\n","먼저 http://mng.bz/0tIo 에서 IMDB 원본 데이터셋을 다운로드하고 압축을 해제합니다.\n","\n","훈련용 리뷰 하나를 문자열 하나로 만들어 훈련 데이터를 문자열의 리스트로 구성해 보죠. 리뷰 레이블(긍정/부정)도 `labels` 리스트로 만들겠습니다:"],"metadata":{"id":"FSk5m6L57ULk"}},{"cell_type":"markdown","source":["https://rfriend.tistory.com/600\n"],"metadata":{"id":"DaQhEwqufYh2"}},{"cell_type":"code","source":["# 현재 작업 경로 확인\n","# 기본 설정은 root입니다.\n","%cd"],"metadata":{"id":"HULL--gE7UJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 드라이브에 aclImdb.zip 파일을 업로드 해놓습니다. (시간 많이 걸리지 않습니다.)\n","# 드라이브를 마운트 합니다.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWX0n6UL7UHC","executionInfo":{"status":"ok","timestamp":1649820357261,"user_tz":-540,"elapsed":16331,"user":{"displayName":"믿을조","userId":"15204951959998370886"}},"outputId":"6c09f42d-fa91-4238-9823-7419cb9fbfc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 이제 zipfile 라이브러리를 사용하여 압축을 해제합니다.\n","# 이 작업은 작업경로(root)에서 진행됩니다.\n","import zipfile\n","\n","# %cd #작업할경로# 로 설정하고 해도 되지만, 드라이브로 설정 시 굉장히 오래걸리고 잘 되지 않습니다.\n","\n","zf = zipfile.ZipFile('drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/datasets/aclImdb.zip')\n","zf.extractall()\n","zf.close()"],"metadata":{"id":"To49HXo_7UDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# 현재 imdb 데이터가 있는 경로는 다음과 같습니다.\n","imdb_dir = './aclImdb'\n","train_dir = os.path.join(imdb_dir, 'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(train_dir, label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname), encoding='utf8')\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)"],"metadata":{"id":"9K8y7_gK7UBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100  # 100개 단어 이후는 버립니다\n","training_samples = 200  # 훈련 샘플은 200개입니다\n","validation_samples = 10000  # 검증 샘플은 10,000개입니다\n","max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=maxlen)\n","\n","labels = np.asarray(labels)\n","print('데이터 텐서의 크기:', data.shape)\n","print('레이블 텐서의 크기:', labels.shape)\n","\n","# 데이터를 훈련 세트와 검증 세트로 분할합니다.\n","# 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후에 긍정 샘플이 옵니다) \n","# 먼저 데이터를 섞습니다.\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXGyGVxc83NV","executionInfo":{"status":"ok","timestamp":1649820385554,"user_tz":-540,"elapsed":11148,"user":{"displayName":"믿을조","userId":"15204951959998370886"}},"outputId":"226f0507-38b2-4b88-c337-2bbeb806e135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["88582개의 고유한 토큰을 찾았습니다.\n","데이터 텐서의 크기: (25000, 100)\n","레이블 텐서의 크기: (25000,)\n"]}]},{"cell_type":"markdown","source":["### GloVe 단어 임베딩 내려받기\n","\n","https://nlp.stanford.edu/projects/glove 에서 2014년 영문 위키피디아를 사용해 사전에 계산된 임베딩을 내려받습니다. 이 파일의 이름은 glove.6B.zip이고 압축 파일 크기는 823MB입니다. 400,000만개의 단어(또는 단어가 아닌 토큰)에 대한 100차원의 임베딩 벡터를 포함하고 있습니다. datasets 폴더 아래에 파일 압축을 해제합니다.\n"],"metadata":{"id":"TJcB35DlrFpM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZVM0szehq_j","executionInfo":{"status":"ok","timestamp":1649816076017,"user_tz":-540,"elapsed":8507,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"1e416028-c23c-412b-a940-1cc88daec6ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["400000개의 단어 벡터\n"]}],"source":["import os\n","import numpy as np\n","glove_dir = 'drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/datasets/'\n","\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","print(f'{len(embeddings_index)}개의 단어 벡터')\n"]},{"cell_type":"markdown","source":["200개의 샘플을 학습한 후 영화 리뷰를 분류"],"metadata":{"id":"fNOeUUNDudbz"}},{"cell_type":"code","source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"10s9bnsPs1kC","executionInfo":{"status":"error","timestamp":1649816252654,"user_tz":-540,"elapsed":411,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"676f0829-3d9c-4228-afef-905155711cb1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-5f0d8aa4f20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"]}]}]}